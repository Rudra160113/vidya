# File location: vidya/core/vidya_brain.py

import logging
import random
import string
import datetime
import requests
from vidya.services.supabase_handler import SupabaseHandler
from vidya.services.email_handler import EmailHandler
from vidya.services.web_crawler import WebCrawler

class VidyaBrain:
    """
    The core logic of the Vidya AI assistant, handling conversation,
    history management, and integration with other services.
    """
    def __init__(self, gemini_api_key, hf_api_key, supabase_handler: SupabaseHandler, email_handler: EmailHandler):
        self.gemini_api_key = gemini_api_key
        self.hf_api_key = hf_api_key
        self.supabase_handler = supabase_handler
        self.email_handler = email_handler
        self.conversation_history = {} # A simple in-memory history
        logging.info("VidyaBrain initialized.")

    def process_input(self, user_input, user_id):
        """
        Processes user input and returns an AI-generated response.
        """
        # Log the user's interaction
        self.supabase_handler.log_interaction(user_id, 'user', user_input)
        
        # Get historical context for the user
        history_list = self.supabase_handler.get_history(user_id)
        context = " ".join([item['message'] for item in history_list])

        # Generate a response using a combined approach (Gemini for a detailed response)
        try:
            # First, check the crawled data for relevant information
            search_results = self.supabase_handler.search_crawled_data(user_input)
            if search_results:
                search_content = " ".join([result['content'] for result in search_results])
                full_prompt = f"Using this information: '{search_content}', and this conversation context: '{context}', please answer the user's question: '{user_input}'"
            else:
                full_prompt = f"Conversation context: '{context}'. User's question: '{user_input}'"

            # Use Gemini API to generate a response
            gemini_response = self._get_gemini_response(full_prompt)
            response_text = gemini_response
            logging.info(f"AI response generated by Gemini: {response_text}")

            # Fallback to Hugging Face model if Gemini fails
            if not response_text:
                logging.warning("Gemini failed. Falling back to Hugging Face model.")
                response_text = self._get_huggingface_response(full_prompt)
                logging.info(f"AI response generated by Hugging Face: {response_text}")

        except Exception as e:
            logging.error(f"Failed to generate AI response: {e}")
            response_text = "I am currently unable to process your request. Please try again later."
        
        # Log the AI's response
        self.supabase_handler.log_interaction(user_id, 'vidya', response_text)
        return response_text

    def _get_gemini_response(self, prompt):
        """Fetches a response from the Google Gemini API."""
        # This is a placeholder. You would need to use a library like `google-generativeai`
        # or make a direct API call here.
        #
        # Example using the requests library:
        # url = "https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent"
        # headers = {"Content-Type": "application/json"}
        # data = {
        #     "contents": [{"parts": [{"text": prompt}]}]
        # }
        # response = requests.post(url, headers=headers, json=data, params={"key": self.gemini_api_key})
        # return response.json()['candidates'][0]['content']['parts'][0]['text']
        #
        # For now, we return a simple placeholder response.
        logging.warning("Using a placeholder response for Gemini. Please implement the actual API call.")
        return "This is a placeholder response from the Gemini model."

    def _get_huggingface_response(self, prompt):
        """Fetches a response from a Hugging Face model."""
        # This is a placeholder. You would use a Hugging Face Inference API.
        #
        # Example using requests:
        # API_URL = "https://api-inference.huggingface.co/models/google/flan-t5-large"
        # headers = {"Authorization": f"Bearer {self.hf_api_key}"}
        # payload = {"inputs": prompt}
        # response = requests.post(API_URL, headers=headers, json=payload)
        # return response.json()[0]['generated_text']
        #
        # For now, we return a simple placeholder response.
        logging.warning("Using a placeholder response for Hugging Face. Please implement the actual API call.")
        return "This is a placeholder response from the Hugging Face model."

    def get_history(self, user_id):
        """Retrieves and formats a user's full conversation history."""
        history = self.supabase_handler.get_history(user_id)
        formatted_history = ""
        for record in history:
            formatted_history += f"{record['created_at']} - {record['source']}: {record['message']}\n"
        return formatted_history

    def send_otp_for_history(self, user_id: str, email: str) -> bool:
        """Generates and sends a one-time password for history access."""
        otp = ''.join(random.choices(string.digits, k=6))
        
        # Store the OTP in Supabase
        if not self.supabase_handler.store_otp(user_id, email, otp):
            return False

        # Send the OTP via email
        subject = "Vidya AI History Access OTP"
        body = f"Hello,\n\nYour one-time password for accessing your Vidya AI history is: {otp}\n\nThis OTP is valid for 25 minutes.\n\nThank you,\nVidya Team"
        return self.email_handler.send_email(email, subject, body)

    def verify_otp(self, user_id: str, otp: str) -> bool:
        """Verifies a one-time password against the stored value in Supabase."""
        return self.supabase_handler.verify_otp(user_id, otp)
