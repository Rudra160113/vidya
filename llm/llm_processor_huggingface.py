# File location: vidya/llm/llm_processor_huggingface.py

import logging
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

class LLMProcessorHuggingFace:
    """
    Processes user input using a Hugging Face model for text generation.
    """
    def __init__(self, model_name: str = "HuggingFaceH4/zephyr-7b-alpha"):
        self.model_name = model_name
        self._setup_pipeline()
        logging.info("LLMProcessorHuggingFace initialized.")

    def _setup_pipeline(self):
        """Sets up the Hugging Face text generation pipeline."""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer
            )
            logging.info(f"Hugging Face text generation pipeline loaded with model '{self.model_name}'.")
        except Exception as e:
            logging.error(f"Failed to load Hugging Face LLM model '{self.model_name}': {e}")
            self.generator = None

    def generate_response(self, prompt: str) -> str:
        """
        Generates a text response from the Hugging Face model.

        Args:
            prompt (str): The user's input.

        Returns:
            str: The generated response from the LLM.
        """
        if not self.generator:
            return "I am unable to process your request. My LLM model is not configured."

        try:
            # Generate response based on the prompt
            response = self.generator(
                prompt,
                max_new_tokens=50,
                num_return_sequences=1,
                eos_token_id=self.generator.tokenizer.eos_token_id
            )
            logging.info("Response generated by Hugging Face model.")
            # Extract the generated text, removing the original prompt
            generated_text = response[0]['generated_text'][len(prompt):].strip()
            return generated_text
        except Exception as e:
            logging.error(f"Error generating response from Hugging Face model: {e}")
            return "I'm sorry, I encountered an error. Please try again."
